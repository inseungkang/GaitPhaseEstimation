{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JlyjEIbEjQDF"
   },
   "source": [
    "## LSTM for Gait Phase Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cM3GEQzjjQDG"
   },
   "source": [
    "The purpose of this notebook is to train LSTM models to estimate a user's gait phase on both legs, independent of the locomotion mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtMup6itjQDH"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from lstm_modules import *\n",
    "from convolutional_nn import *\n",
    "from data_processing import *\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7lV9YpljQDN"
   },
   "outputs": [],
   "source": [
    "# Seed Random Number Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnLs0X8bsv4D"
   },
   "source": [
    "### Set Up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TqylDhFsv4G"
   },
   "outputs": [],
   "source": [
    "sensors = ['leftJointPosition', 'rightJointPosition', 'leftJointVelocity',\n",
    "           'rightJointVelocity', 'imuGyroX', 'imuGyroY', 'imuGyroZ', 'imuAccX',\n",
    "           'imuAccY', 'imuAccZ']\n",
    "\n",
    "# Produce 1 label file for each trial and store them in ../labels folder\n",
    "data = import_data(sensors)\n",
    "\n",
    "left_joint_positions, right_joint_positions = extract_joint_positions(data)\n",
    "labels = []\n",
    "for i in range(5):\n",
    "    filename = \"labels/label_trial{}.txt\".format(i+1)\n",
    "    left_gp_x, left_gp_y = label_vectors(left_joint_positions[i])\n",
    "    right_gp_x, right_gp_y = label_vectors(right_joint_positions[i])\n",
    "    labeled_gp = pd.DataFrame({'leftGaitPhase_x': left_gp_x, 'leftGaitPhase_y': left_gp_y,\n",
    "                               'rightGaitPhase_x': right_gp_x, 'rightGaitPhase_y': right_gp_y})\n",
    "    labels.append(labeled_gp)\n",
    "\n",
    "# Combine the data and the labels\n",
    "for d, l in zip(data, labels):\n",
    "    d[l.columns] = l\n",
    "\n",
    "# Creat a list of cut_indicies for each trial\n",
    "cut_indicies_list = []\n",
    "for i in range(5):\n",
    "    cut_indicies_list.append(find_cutting_indices(left_joint_positions[i], \n",
    "    right_joint_positions[i]))\n",
    "\n",
    "# Cut the standing data and store files into ../features folder\n",
    "data_list = cnn_cut_data(data, cut_indicies_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8KMUwS-ZjQDT"
   },
   "source": [
    "### Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uy-ab61Fk78E"
   },
   "outputs": [],
   "source": [
    "hyperparam_space = {\n",
    "    'window_size': [120],\n",
    "    'lstm': {\n",
    "      'units': [10],\n",
    "      'activation': ['relu'],\n",
    "      'recurrent_regularizer': ['l2'],\n",
    "      'kernel_regularizer': ['l2']\n",
    "    },\n",
    "    'dense': {\n",
    "        'activation': ['tanh'],\n",
    "        'kernel_regularizer': ['l2']\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'loss': ['mean_absolute_error'],\n",
    "        'optimizer': ['adam']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k8HSJZSnr7o9"
   },
   "outputs": [],
   "source": [
    "def get_model_configs(hyperparam_space):\n",
    "    model_configs = []\n",
    "    for window_size in hyperparam_space['window_size']:\n",
    "        lstm_params = list(hyperparam_space['lstm'].keys())\n",
    "\n",
    "        lstm_possibilities = []\n",
    "\n",
    "        for param in lstm_params:\n",
    "            lstm_possibilities.append(hyperparam_space['lstm'][param])\n",
    "\n",
    "        lstm_config_tuples = itertools.product(*lstm_possibilities)\n",
    "        lstm_configs = []\n",
    "        for config in lstm_config_tuples:\n",
    "            lstm_config = {}\n",
    "            for i, value in enumerate(config):\n",
    "                lstm_config[lstm_params[i]] = value\n",
    "            lstm_configs.append(lstm_config)\n",
    "\n",
    "        dense_params = list(hyperparam_space['dense'].keys())\n",
    "\n",
    "        dense_possibilities = []\n",
    "\n",
    "        for param in dense_params:\n",
    "            dense_possibilities.append(hyperparam_space['dense'][param])\n",
    "\n",
    "        dense_config_tuples = itertools.product(*dense_possibilities)\n",
    "        dense_configs = []\n",
    "        for config in dense_config_tuples:\n",
    "            dense_config = {}\n",
    "            for i, value in enumerate(config):\n",
    "                dense_config[dense_params[i]] = value\n",
    "            dense_configs.append(dense_config)\n",
    "\n",
    "        optim_params = list(hyperparam_space['optimizer'].keys())\n",
    "\n",
    "        optim_possibilities = []\n",
    "\n",
    "        for param in optim_params:\n",
    "            optim_possibilities.append(hyperparam_space['optimizer'][param])\n",
    "\n",
    "        optim_config_tuples = itertools.product(*optim_possibilities)\n",
    "        optim_configs = []\n",
    "        for config in optim_config_tuples:\n",
    "            optim_config = {}\n",
    "            for i, value in enumerate(config):\n",
    "                optim_config[optim_params[i]] = value\n",
    "            optim_configs.append(optim_config)\n",
    "\n",
    "        possible_configs = itertools.product(lstm_configs, dense_configs, optim_configs)\n",
    "        config_count = 0\n",
    "        for config in possible_configs:\n",
    "            config_count += 1\n",
    "            model_configs.append({\n",
    "                'window_size': window_size,\n",
    "                'lstm': config[0],\n",
    "                'dense': config[1],\n",
    "                'optimizer': config[2]\n",
    "            })\n",
    "    return model_configs            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rnSCRfsq4CEo",
    "outputId": "3f6b8e97-4a51-478d-81b5-7401881cf5fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_configs = get_model_configs(hyperparam_space)\n",
    "print(len(hyperparameter_configs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "colab_type": "code",
    "id": "3vJNubQa4iFu",
    "outputId": "4dffff85-2457-4a85-822e-e3041533f74e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization (Normalization (None, 120, 10)           21        \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 44        \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 884\n",
      "Non-trainable params: 21\n",
      "_________________________________________________________________\n",
      "Train on 27496 samples, validate on 6875 samples\n",
      "Epoch 1/30\n",
      "27496/27496 [==============================] - 17s 601us/sample - loss: 1.0741 - val_loss: 0.9868\n",
      "Epoch 2/30\n",
      "27496/27496 [==============================] - 17s 608us/sample - loss: 0.8573 - val_loss: 0.7553\n",
      "Epoch 3/30\n",
      "27496/27496 [==============================] - 19s 706us/sample - loss: 0.6552 - val_loss: 0.5925\n",
      "Epoch 4/30\n",
      "27496/27496 [==============================] - 15s 551us/sample - loss: 0.5537 - val_loss: 0.5345\n",
      "Epoch 5/30\n",
      "27496/27496 [==============================] - 16s 571us/sample - loss: 0.5092 - val_loss: 0.5066\n",
      "Epoch 6/30\n",
      "27496/27496 [==============================] - 16s 582us/sample - loss: 0.4850 - val_loss: 0.4819\n",
      "Epoch 7/30\n",
      "27496/27496 [==============================] - 16s 578us/sample - loss: 0.4670 - val_loss: 0.4647\n",
      "Epoch 8/30\n",
      "27496/27496 [==============================] - 16s 580us/sample - loss: 0.4477 - val_loss: 0.4478\n",
      "Epoch 9/30\n",
      "27496/27496 [==============================] - 16s 582us/sample - loss: 0.4295 - val_loss: 0.4296\n",
      "Epoch 10/30\n",
      "27496/27496 [==============================] - 16s 567us/sample - loss: 0.4034 - val_loss: 0.3987\n",
      "Epoch 11/30\n",
      "27496/27496 [==============================] - 18s 660us/sample - loss: 0.3809 - val_loss: 0.3846\n",
      "Epoch 12/30\n",
      "27496/27496 [==============================] - 15s 529us/sample - loss: 0.3642 - val_loss: 0.3627\n",
      "Epoch 13/30\n",
      "27496/27496 [==============================] - 15s 541us/sample - loss: 0.3481 - val_loss: 0.3486\n",
      "Epoch 14/30\n",
      "27496/27496 [==============================] - 15s 562us/sample - loss: 0.3346 - val_loss: 0.3388\n",
      "Epoch 15/30\n",
      "27496/27496 [==============================] - 15s 548us/sample - loss: 0.3264 - val_loss: 0.3300\n",
      "Epoch 16/30\n",
      "27496/27496 [==============================] - 14s 509us/sample - loss: 0.3179 - val_loss: 0.3223\n",
      "Epoch 17/30\n",
      "27496/27496 [==============================] - 14s 525us/sample - loss: 0.3104 - val_loss: 0.3185\n",
      "Epoch 18/30\n",
      "27496/27496 [==============================] - 14s 512us/sample - loss: 0.3046 - val_loss: 0.3120\n",
      "Epoch 19/30\n",
      "27496/27496 [==============================] - 14s 510us/sample - loss: 0.2982 - val_loss: 0.3053\n",
      "Epoch 20/30\n",
      "27496/27496 [==============================] - 14s 495us/sample - loss: 0.2922 - val_loss: 0.3012\n",
      "Epoch 21/30\n",
      "27496/27496 [==============================] - 17s 602us/sample - loss: 0.2866 - val_loss: 0.2965\n",
      "Epoch 22/30\n",
      "27496/27496 [==============================] - 16s 580us/sample - loss: 0.2818 - val_loss: 0.2893\n",
      "Epoch 23/30\n",
      "27496/27496 [==============================] - 16s 574us/sample - loss: 0.2769 - val_loss: 0.2861\n",
      "Epoch 24/30\n",
      "27496/27496 [==============================] - 17s 606us/sample - loss: 0.2722 - val_loss: 0.2814\n",
      "Epoch 25/30\n",
      "27496/27496 [==============================] - 13s 490us/sample - loss: 0.2673 - val_loss: 0.2758\n",
      "Epoch 26/30\n",
      "27496/27496 [==============================] - 16s 581us/sample - loss: 0.2622 - val_loss: 0.2705\n",
      "Epoch 27/30\n",
      "27496/27496 [==============================] - 16s 573us/sample - loss: 0.2586 - val_loss: 0.2746\n",
      "Epoch 28/30\n",
      "27496/27496 [==============================] - 16s 567us/sample - loss: 0.2552 - val_loss: 0.2631\n",
      "Epoch 29/30\n",
      "27496/27496 [==============================] - 15s 552us/sample - loss: 0.2502 - val_loss: 0.2593\n",
      "Epoch 30/30\n",
      "27496/27496 [==============================] - 18s 663us/sample - loss: 0.2462 - val_loss: 0.2602\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization_1 (Normalizati (None, 120, 10)           21        \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 44        \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 884\n",
      "Non-trainable params: 21\n",
      "_________________________________________________________________\n",
      "Train on 26790 samples, validate on 6698 samples\n",
      "Epoch 1/30\n",
      "26790/26790 [==============================] - 20s 733us/sample - loss: 0.7130 - val_loss: 0.4449\n",
      "Epoch 2/30\n",
      "26790/26790 [==============================] - 16s 583us/sample - loss: 0.2905 - val_loss: 0.2297\n",
      "Epoch 3/30\n",
      "26790/26790 [==============================] - 17s 628us/sample - loss: 0.1846 - val_loss: 0.1740\n",
      "Epoch 4/30\n",
      "26790/26790 [==============================] - 16s 596us/sample - loss: 0.1443 - val_loss: 0.1493\n",
      "Epoch 5/30\n",
      "26790/26790 [==============================] - 15s 546us/sample - loss: 0.1225 - val_loss: 0.1291\n",
      "Epoch 6/30\n",
      "26790/26790 [==============================] - 15s 564us/sample - loss: 0.1086 - val_loss: 0.1211\n",
      "Epoch 7/30\n",
      "26790/26790 [==============================] - 16s 595us/sample - loss: 0.0988 - val_loss: 0.1122\n",
      "Epoch 8/30\n",
      "26790/26790 [==============================] - 16s 606us/sample - loss: 0.0912 - val_loss: 0.1063\n",
      "Epoch 9/30\n",
      "26790/26790 [==============================] - 15s 560us/sample - loss: 0.0853 - val_loss: 0.0956\n",
      "Epoch 10/30\n",
      "26790/26790 [==============================] - 15s 565us/sample - loss: 0.0797 - val_loss: 0.0945\n",
      "Epoch 11/30\n",
      "26790/26790 [==============================] - 15s 557us/sample - loss: 0.0762 - val_loss: 0.0920\n",
      "Epoch 12/30\n",
      "26790/26790 [==============================] - 15s 568us/sample - loss: 0.0736 - val_loss: 0.0912\n",
      "Epoch 13/30\n",
      "26790/26790 [==============================] - 15s 575us/sample - loss: 0.0710 - val_loss: 0.0915\n",
      "Epoch 14/30\n",
      "26790/26790 [==============================] - 16s 579us/sample - loss: 0.0699 - val_loss: 0.0855\n",
      "Epoch 15/30\n",
      "26790/26790 [==============================] - 17s 650us/sample - loss: 0.0683 - val_loss: 0.0868\n",
      "Epoch 16/30\n",
      "26790/26790 [==============================] - 15s 573us/sample - loss: 0.0663 - val_loss: 0.0837\n",
      "Epoch 17/30\n",
      "26790/26790 [==============================] - 12s 464us/sample - loss: 0.0659 - val_loss: 0.0830\n",
      "Epoch 18/30\n",
      "26790/26790 [==============================] - 14s 513us/sample - loss: 0.0644 - val_loss: 0.0844\n",
      "Epoch 19/30\n",
      "26790/26790 [==============================] - 14s 539us/sample - loss: 0.0646 - val_loss: 0.0830\n",
      "Epoch 20/30\n",
      "26790/26790 [==============================] - 14s 540us/sample - loss: 0.0641 - val_loss: 0.0822\n",
      "Epoch 21/30\n",
      "26790/26790 [==============================] - 15s 577us/sample - loss: 0.0628 - val_loss: 0.0848\n",
      "Epoch 22/30\n",
      "26790/26790 [==============================] - 14s 541us/sample - loss: 0.0628 - val_loss: 0.0834\n",
      "Epoch 23/30\n",
      "26790/26790 [==============================] - 16s 581us/sample - loss: 0.0617 - val_loss: 0.0816\n",
      "Epoch 24/30\n",
      "26790/26790 [==============================] - 16s 582us/sample - loss: 0.0609 - val_loss: 0.0843\n",
      "Epoch 25/30\n",
      "26790/26790 [==============================] - 15s 562us/sample - loss: 0.0604 - val_loss: 0.0820\n",
      "Epoch 26/30\n",
      "26790/26790 [==============================] - 15s 564us/sample - loss: 0.0598 - val_loss: 0.0792\n",
      "Epoch 27/30\n",
      "26790/26790 [==============================] - 14s 519us/sample - loss: 0.0593 - val_loss: 0.0779\n",
      "Epoch 28/30\n",
      "26790/26790 [==============================] - 16s 587us/sample - loss: 0.0588 - val_loss: 0.0820\n",
      "Epoch 29/30\n",
      "26790/26790 [==============================] - 15s 563us/sample - loss: 0.0590 - val_loss: 0.0816\n",
      "Epoch 30/30\n",
      "26790/26790 [==============================] - 16s 586us/sample - loss: 0.0581 - val_loss: 0.0776\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization_2 (Normalizati (None, 120, 10)           21        \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 44        \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 884\n",
      "Non-trainable params: 21\n",
      "_________________________________________________________________\n",
      "Train on 27548 samples, validate on 6887 samples\n",
      "Epoch 1/30\n",
      "27548/27548 [==============================] - 20s 732us/sample - loss: 0.4897 - val_loss: 0.2793\n",
      "Epoch 2/30\n",
      "27548/27548 [==============================] - 16s 583us/sample - loss: 0.1925 - val_loss: 0.1617\n",
      "Epoch 3/30\n",
      "27548/27548 [==============================] - 16s 594us/sample - loss: 0.1295 - val_loss: 0.1305\n",
      "Epoch 4/30\n",
      "27548/27548 [==============================] - 15s 542us/sample - loss: 0.1070 - val_loss: 0.1160\n",
      "Epoch 5/30\n",
      "27548/27548 [==============================] - 15s 544us/sample - loss: 0.0963 - val_loss: 0.1096\n",
      "Epoch 6/30\n",
      "27548/27548 [==============================] - 12s 420us/sample - loss: 0.0892 - val_loss: 0.1059\n",
      "Epoch 7/30\n",
      "27548/27548 [==============================] - 12s 445us/sample - loss: 0.0838 - val_loss: 0.0965\n",
      "Epoch 8/30\n",
      "27548/27548 [==============================] - 12s 429us/sample - loss: 0.0809 - val_loss: 0.0958\n",
      "Epoch 9/30\n",
      "27548/27548 [==============================] - 11s 408us/sample - loss: 0.0784 - val_loss: 0.0976\n",
      "Epoch 10/30\n",
      "27548/27548 [==============================] - 11s 387us/sample - loss: 0.0759 - val_loss: 0.0946\n",
      "Epoch 11/30\n",
      "27548/27548 [==============================] - 10s 375us/sample - loss: 0.0750 - val_loss: 0.0887\n",
      "Epoch 12/30\n",
      "27548/27548 [==============================] - 10s 377us/sample - loss: 0.0722 - val_loss: 0.0921\n",
      "Epoch 13/30\n",
      "27548/27548 [==============================] - 10s 371us/sample - loss: 0.0720 - val_loss: 0.0880\n",
      "Epoch 14/30\n",
      "27548/27548 [==============================] - 10s 381us/sample - loss: 0.0705 - val_loss: 0.0866\n",
      "Epoch 15/30\n",
      "27548/27548 [==============================] - 10s 374us/sample - loss: 0.0697 - val_loss: 0.0919\n",
      "Epoch 16/30\n",
      "27548/27548 [==============================] - 11s 383us/sample - loss: 0.0689 - val_loss: 0.0861\n",
      "Epoch 17/30\n",
      "27548/27548 [==============================] - 10s 379us/sample - loss: 0.0679 - val_loss: 0.0823\n",
      "Epoch 18/30\n",
      "27548/27548 [==============================] - 10s 370us/sample - loss: 0.0668 - val_loss: 0.0830\n",
      "Epoch 19/30\n",
      "27548/27548 [==============================] - 10s 376us/sample - loss: 0.0670 - val_loss: 0.0849\n",
      "Epoch 20/30\n",
      "27548/27548 [==============================] - 11s 393us/sample - loss: 0.0658 - val_loss: 0.0851\n",
      "Epoch 21/30\n",
      "27548/27548 [==============================] - 11s 388us/sample - loss: 0.0655 - val_loss: 0.0830\n",
      "Epoch 22/30\n",
      "27548/27548 [==============================] - 10s 366us/sample - loss: 0.0649 - val_loss: 0.0880\n",
      "Epoch 23/30\n",
      "27548/27548 [==============================] - 10s 361us/sample - loss: 0.0656 - val_loss: 0.0801\n",
      "Epoch 24/30\n",
      "27548/27548 [==============================] - 10s 366us/sample - loss: 0.0636 - val_loss: 0.0792\n",
      "Epoch 25/30\n",
      "27548/27548 [==============================] - 10s 363us/sample - loss: 0.0633 - val_loss: 0.0785\n",
      "Epoch 26/30\n",
      "27548/27548 [==============================] - 10s 368us/sample - loss: 0.0634 - val_loss: 0.0778\n",
      "Epoch 27/30\n",
      "27548/27548 [==============================] - 10s 368us/sample - loss: 0.0630 - val_loss: 0.0789\n",
      "Epoch 28/30\n",
      "27548/27548 [==============================] - 10s 367us/sample - loss: 0.0621 - val_loss: 0.0794\n",
      "Epoch 29/30\n",
      "27548/27548 [==============================] - 10s 375us/sample - loss: 0.0623 - val_loss: 0.0792 \n",
      "Epoch 30/30\n",
      "27548/27548 [==============================] - 11s 387us/sample - loss: 0.0616 - val_loss: 0.0781\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization_3 (Normalizati (None, 120, 10)           21        \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 44        \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 884\n",
      "Non-trainable params: 21\n",
      "_________________________________________________________________\n",
      "Train on 26467 samples, validate on 6617 samples\n",
      "Epoch 1/30\n",
      "26467/26467 [==============================] - 11s 420us/sample - loss: 0.5638 - val_loss: 0.3045\n",
      "Epoch 2/30\n",
      "26467/26467 [==============================] - 10s 363us/sample - loss: 0.2271 - val_loss: 0.1915\n",
      "Epoch 3/30\n",
      "26467/26467 [==============================] - 10s 370us/sample - loss: 0.1558 - val_loss: 0.1447\n",
      "Epoch 4/30\n",
      "26467/26467 [==============================] - 10s 381us/sample - loss: 0.1215 - val_loss: 0.1232\n",
      "Epoch 5/30\n",
      "26467/26467 [==============================] - 9s 351us/sample - loss: 0.1019 - val_loss: 0.1171\n",
      "Epoch 6/30\n",
      "26467/26467 [==============================] - 9s 343us/sample - loss: 0.0915 - val_loss: 0.1049\n",
      "Epoch 7/30\n",
      "26467/26467 [==============================] - 9s 348us/sample - loss: 0.0851 - val_loss: 0.1029\n",
      "Epoch 8/30\n",
      "26467/26467 [==============================] - 9s 343us/sample - loss: 0.0806 - val_loss: 0.0939\n",
      "Epoch 9/30\n",
      "26467/26467 [==============================] - 9s 353us/sample - loss: 0.0776 - val_loss: 0.0935\n",
      "Epoch 10/30\n",
      "26467/26467 [==============================] - 10s 383us/sample - loss: 0.0752 - val_loss: 0.0928\n",
      "Epoch 11/30\n",
      "26467/26467 [==============================] - 13s 505us/sample - loss: 0.0737 - val_loss: 0.0888\n",
      "Epoch 12/30\n",
      "26467/26467 [==============================] - 11s 430us/sample - loss: 0.0717 - val_loss: 0.0901\n",
      "Epoch 13/30\n",
      "26467/26467 [==============================] - 10s 391us/sample - loss: 0.0709 - val_loss: 0.0852\n",
      "Epoch 14/30\n",
      "26467/26467 [==============================] - 10s 379us/sample - loss: 0.0691 - val_loss: 0.0885\n",
      "Epoch 15/30\n",
      "26467/26467 [==============================] - 11s 424us/sample - loss: 0.0682 - val_loss: 0.0861\n",
      "Epoch 16/30\n",
      "26467/26467 [==============================] - 10s 392us/sample - loss: 0.0682 - val_loss: 0.0837\n",
      "Epoch 17/30\n",
      "26467/26467 [==============================] - 11s 405us/sample - loss: 0.0663 - val_loss: 0.0835\n",
      "Epoch 18/30\n",
      "26467/26467 [==============================] - 10s 372us/sample - loss: 0.0657 - val_loss: 0.0863\n",
      "Epoch 19/30\n",
      "26467/26467 [==============================] - 9s 339us/sample - loss: 0.0649 - val_loss: 0.0812\n",
      "Epoch 20/30\n",
      "26467/26467 [==============================] - 9s 340us/sample - loss: 0.0658 - val_loss: 0.0844\n",
      "Epoch 21/30\n",
      "26467/26467 [==============================] - 9s 345us/sample - loss: 0.0640 - val_loss: 0.0818\n",
      "Epoch 22/30\n",
      "26467/26467 [==============================] - 14s 519us/sample - loss: 0.0636 - val_loss: 0.0827\n",
      "Epoch 23/30\n",
      "26467/26467 [==============================] - 12s 440us/sample - loss: 0.0628 - val_loss: 0.0811\n",
      "Epoch 24/30\n",
      "26467/26467 [==============================] - 9s 355us/sample - loss: 0.0622 - val_loss: 0.0826\n",
      "Epoch 25/30\n",
      "26467/26467 [==============================] - 9s 353us/sample - loss: 0.0630 - val_loss: 0.0795\n",
      "Epoch 26/30\n",
      "26467/26467 [==============================] - 10s 361us/sample - loss: 0.0621 - val_loss: 0.0842\n",
      "Epoch 27/30\n",
      "26467/26467 [==============================] - 10s 359us/sample - loss: 0.0630 - val_loss: 0.0788\n",
      "Epoch 28/30\n",
      "26467/26467 [==============================] - 10s 374us/sample - loss: 0.0622 - val_loss: 0.0803\n",
      "Epoch 29/30\n",
      "26467/26467 [==============================] - 10s 359us/sample - loss: 0.0616 - val_loss: 0.0805\n",
      "Epoch 30/30\n",
      "26467/26467 [==============================] - 9s 348us/sample - loss: 0.0609 - val_loss: 0.0821\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization_4 (Normalizati (None, 120, 10)           21        \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 44        \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 884\n",
      "Non-trainable params: 21\n",
      "_________________________________________________________________\n",
      "Train on 27513 samples, validate on 6879 samples\n",
      "Epoch 1/30\n",
      "27513/27513 [==============================] - 12s 428us/sample - loss: 0.6347 - val_loss: 0.3725\n",
      "Epoch 2/30\n",
      "27513/27513 [==============================] - 10s 381us/sample - loss: 0.2386 - val_loss: 0.1859\n",
      "Epoch 3/30\n",
      "27513/27513 [==============================] - 10s 369us/sample - loss: 0.1520 - val_loss: 0.1492\n",
      "Epoch 4/30\n",
      "27513/27513 [==============================] - 11s 396us/sample - loss: 0.1232 - val_loss: 0.1285\n",
      "Epoch 5/30\n",
      "27513/27513 [==============================] - 11s 407us/sample - loss: 0.1081 - val_loss: 0.1166\n",
      "Epoch 6/30\n",
      "27513/27513 [==============================] - 11s 405us/sample - loss: 0.0989 - val_loss: 0.1137\n",
      "Epoch 7/30\n",
      "27513/27513 [==============================] - 10s 365us/sample - loss: 0.0941 - val_loss: 0.1047\n",
      "Epoch 8/30\n",
      "27513/27513 [==============================] - 10s 375us/sample - loss: 0.0891 - val_loss: 0.1027\n",
      "Epoch 9/30\n",
      "27513/27513 [==============================] - 13s 474us/sample - loss: 0.0861 - val_loss: 0.1003\n",
      "Epoch 10/30\n",
      "27513/27513 [==============================] - 19s 697us/sample - loss: 0.0841 - val_loss: 0.0996\n",
      "Epoch 11/30\n",
      "27513/27513 [==============================] - 13s 481us/sample - loss: 0.0815 - val_loss: 0.0968\n",
      "Epoch 12/30\n",
      "27513/27513 [==============================] - 15s 553us/sample - loss: 0.0797 - val_loss: 0.0921\n",
      "Epoch 13/30\n",
      "27513/27513 [==============================] - 17s 603us/sample - loss: 0.0775 - val_loss: 0.0913\n",
      "Epoch 14/30\n",
      "27513/27513 [==============================] - 14s 507us/sample - loss: 0.0761 - val_loss: 0.0908\n",
      "Epoch 15/30\n",
      "27513/27513 [==============================] - 11s 414us/sample - loss: 0.0745 - val_loss: 0.0898\n",
      "Epoch 16/30\n",
      "27513/27513 [==============================] - 11s 394us/sample - loss: 0.0741 - val_loss: 0.0870\n",
      "Epoch 17/30\n",
      "27513/27513 [==============================] - 11s 396us/sample - loss: 0.0730 - val_loss: 0.0884\n",
      "Epoch 18/30\n",
      "27513/27513 [==============================] - 12s 438us/sample - loss: 0.0716 - val_loss: 0.0878\n",
      "Epoch 19/30\n",
      "27513/27513 [==============================] - 12s 448us/sample - loss: 0.0705 - val_loss: 0.0854\n",
      "Epoch 20/30\n",
      "27513/27513 [==============================] - 12s 418us/sample - loss: 0.0701 - val_loss: 0.0887\n",
      "Epoch 21/30\n",
      "27513/27513 [==============================] - 10s 367us/sample - loss: 0.0688 - val_loss: 0.0854\n",
      "Epoch 22/30\n",
      "27513/27513 [==============================] - 10s 360us/sample - loss: 0.0685 - val_loss: 0.0823\n",
      "Epoch 23/30\n",
      "27513/27513 [==============================] - 11s 388us/sample - loss: 0.0672 - val_loss: 0.0843\n",
      "Epoch 24/30\n",
      "27513/27513 [==============================] - 10s 358us/sample - loss: 0.0668 - val_loss: 0.0836\n",
      "Epoch 25/30\n",
      "27513/27513 [==============================] - 10s 354us/sample - loss: 0.0666 - val_loss: 0.0827\n",
      "Epoch 26/30\n",
      "27513/27513 [==============================] - 12s 445us/sample - loss: 0.0657 - val_loss: 0.0813\n",
      "Epoch 27/30\n",
      "27513/27513 [==============================] - 11s 403us/sample - loss: 0.0653 - val_loss: 0.0827\n",
      "Epoch 28/30\n",
      "27513/27513 [==============================] - 14s 505us/sample - loss: 0.0647 - val_loss: 0.0825\n",
      "Epoch 29/30\n",
      "27513/27513 [==============================] - 11s 404us/sample - loss: 0.0647 - val_loss: 0.0807\n",
      "Epoch 30/30\n",
      "27513/27513 [==============================] - 11s 410us/sample - loss: 0.0636 - val_loss: 0.0794\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization_5 (Normalizati (None, 120, 10)           21        \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 44        \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 884\n",
      "Non-trainable params: 21\n",
      "_________________________________________________________________\n",
      "Train on 26764 samples, validate on 6691 samples\n",
      "Epoch 1/30\n",
      "26764/26764 [==============================] - 13s 496us/sample - loss: 0.8145 - val_loss: 0.8204\n",
      "Epoch 2/30\n",
      "26764/26764 [==============================] - 12s 435us/sample - loss: 0.6968 - val_loss: 0.5671\n",
      "Epoch 3/30\n",
      "26764/26764 [==============================] - 10s 376us/sample - loss: 0.5035 - val_loss: 0.4762\n",
      "Epoch 4/30\n",
      "26764/26764 [==============================] - 11s 411us/sample - loss: 0.4255 - val_loss: 0.3992\n",
      "Epoch 5/30\n",
      "26764/26764 [==============================] - 11s 422us/sample - loss: 0.3597 - val_loss: 0.3511\n",
      "Epoch 6/30\n",
      "26764/26764 [==============================] - 11s 401us/sample - loss: 0.3042 - val_loss: 0.3024\n",
      "Epoch 7/30\n",
      "26764/26764 [==============================] - 11s 415us/sample - loss: 0.2618 - val_loss: 0.2690\n",
      "Epoch 8/30\n",
      "26764/26764 [==============================] - 11s 408us/sample - loss: 0.2349 - val_loss: 0.2460\n",
      "Epoch 9/30\n",
      "26764/26764 [==============================] - 14s 504us/sample - loss: 0.2196 - val_loss: 0.2317\n",
      "Epoch 10/30\n",
      "26764/26764 [==============================] - 13s 486us/sample - loss: 0.2069 - val_loss: 0.2227\n",
      "Epoch 11/30\n",
      "26764/26764 [==============================] - 13s 476us/sample - loss: 0.1976 - val_loss: 0.2114\n",
      "Epoch 12/30\n",
      "26764/26764 [==============================] - 11s 405us/sample - loss: 0.1889 - val_loss: 0.2012\n",
      "Epoch 13/30\n",
      "26764/26764 [==============================] - 11s 393us/sample - loss: 0.1830 - val_loss: 0.1962\n",
      "Epoch 14/30\n",
      "26764/26764 [==============================] - 10s 367us/sample - loss: 0.1771 - val_loss: 0.1871\n",
      "Epoch 15/30\n",
      "26764/26764 [==============================] - 10s 369us/sample - loss: 0.1697 - val_loss: 0.1843\n",
      "Epoch 16/30\n",
      "26764/26764 [==============================] - 10s 368us/sample - loss: 0.1633 - val_loss: 0.1770\n",
      "Epoch 17/30\n",
      "26764/26764 [==============================] - 10s 369us/sample - loss: 0.1567 - val_loss: 0.1835\n",
      "Epoch 18/30\n",
      "26764/26764 [==============================] - 10s 367us/sample - loss: 0.1517 - val_loss: 0.1666\n",
      "Epoch 19/30\n",
      "26764/26764 [==============================] - 10s 369us/sample - loss: 0.1458 - val_loss: 0.1642\n",
      "Epoch 20/30\n",
      "26764/26764 [==============================] - 10s 378us/sample - loss: 0.1413 - val_loss: 0.1640\n",
      "Epoch 21/30\n",
      "26764/26764 [==============================] - 10s 379us/sample - loss: 0.1364 - val_loss: 0.1583\n",
      "Epoch 22/30\n",
      "26764/26764 [==============================] - 10s 383us/sample - loss: 0.1321 - val_loss: 0.1511\n",
      "Epoch 23/30\n",
      "26764/26764 [==============================] - 13s 478us/sample - loss: 0.1277 - val_loss: 0.1435\n",
      "Epoch 24/30\n",
      "26764/26764 [==============================] - 11s 423us/sample - loss: 0.1234 - val_loss: 0.1414\n",
      "Epoch 25/30\n",
      "26764/26764 [==============================] - 10s 386us/sample - loss: 0.1196 - val_loss: 0.1348\n",
      "Epoch 26/30\n",
      "26764/26764 [==============================] - 10s 382us/sample - loss: 0.1159 - val_loss: 0.1365\n",
      "Epoch 27/30\n",
      "26764/26764 [==============================] - 11s 406us/sample - loss: 0.1125 - val_loss: 0.1290\n",
      "Epoch 28/30\n",
      "26764/26764 [==============================] - 11s 396us/sample - loss: 0.1090 - val_loss: 0.1332\n",
      "Epoch 29/30\n",
      "26764/26764 [==============================] - 10s 363us/sample - loss: 0.1062 - val_loss: 0.1329\n",
      "Epoch 30/30\n",
      "26764/26764 [==============================] - 11s 393us/sample - loss: 0.1038 - val_loss: 0.1198\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization_6 (Normalizati (None, 120, 10)           21        \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 44        \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 884\n",
      "Non-trainable params: 21\n",
      "_________________________________________________________________\n",
      "Train on 27557 samples, validate on 6890 samples\n",
      "Epoch 1/30\n",
      "27557/27557 [==============================] - 12s 442us/sample - loss: 0.7111 - val_loss: 0.4929\n",
      "Epoch 2/30\n",
      "27557/27557 [==============================] - 9s 344us/sample - loss: 0.2937 - val_loss: 0.2141\n",
      "Epoch 3/30\n",
      "27557/27557 [==============================] - 10s 353us/sample - loss: 0.1708 - val_loss: 0.1558\n",
      "Epoch 4/30\n",
      "27557/27557 [==============================] - 10s 369us/sample - loss: 0.1268 - val_loss: 0.1290\n",
      "Epoch 5/30\n",
      "27557/27557 [==============================] - 10s 369us/sample - loss: 0.1055 - val_loss: 0.1166\n",
      "Epoch 6/30\n",
      "27557/27557 [==============================] - 12s 440us/sample - loss: 0.0946 - val_loss: 0.1054\n",
      "Epoch 7/30\n",
      "27557/27557 [==============================] - 13s 454us/sample - loss: 0.0863 - val_loss: 0.1023\n",
      "Epoch 8/30\n",
      "27557/27557 [==============================] - 11s 395us/sample - loss: 0.0823 - val_loss: 0.1002\n",
      "Epoch 9/30\n",
      "27557/27557 [==============================] - 10s 381us/sample - loss: 0.0778 - val_loss: 0.0976\n",
      "Epoch 10/30\n",
      "27557/27557 [==============================] - 11s 393us/sample - loss: 0.0759 - val_loss: 0.0966\n",
      "Epoch 11/30\n",
      "27557/27557 [==============================] - 10s 380us/sample - loss: 0.0736 - val_loss: 0.0929\n",
      "Epoch 12/30\n",
      "27557/27557 [==============================] - 11s 404us/sample - loss: 0.0711 - val_loss: 0.0871\n",
      "Epoch 13/30\n",
      "27557/27557 [==============================] - 10s 374us/sample - loss: 0.0697 - val_loss: 0.0880\n",
      "Epoch 14/30\n",
      "27557/27557 [==============================] - 11s 381us/sample - loss: 0.0690 - val_loss: 0.0945\n",
      "Epoch 15/30\n",
      "27557/27557 [==============================] - 11s 399us/sample - loss: 0.0670 - val_loss: 0.0846\n",
      "Epoch 16/30\n",
      "27557/27557 [==============================] - 11s 415us/sample - loss: 0.0673 - val_loss: 0.0865\n",
      "Epoch 17/30\n",
      "27557/27557 [==============================] - 10s 375us/sample - loss: 0.0663 - val_loss: 0.0819\n",
      "Epoch 18/30\n",
      "27557/27557 [==============================] - 10s 362us/sample - loss: 0.0650 - val_loss: 0.0811\n",
      "Epoch 19/30\n",
      "27557/27557 [==============================] - 10s 373us/sample - loss: 0.0644 - val_loss: 0.0834\n",
      "Epoch 20/30\n",
      "27557/27557 [==============================] - 11s 391us/sample - loss: 0.0638 - val_loss: 0.0822\n",
      "Epoch 21/30\n",
      "27557/27557 [==============================] - 10s 381us/sample - loss: 0.0634 - val_loss: 0.0809\n",
      "Epoch 22/30\n",
      "27557/27557 [==============================] - 11s 392us/sample - loss: 0.0631 - val_loss: 0.0786\n",
      "Epoch 23/30\n",
      "27557/27557 [==============================] - 10s 380us/sample - loss: 0.0616 - val_loss: 0.0821\n",
      "Epoch 24/30\n",
      "27557/27557 [==============================] - 10s 371us/sample - loss: 0.0629 - val_loss: 0.0840\n",
      "Epoch 25/30\n",
      "27557/27557 [==============================] - 11s 392us/sample - loss: 0.0621 - val_loss: 0.0797\n",
      "Epoch 26/30\n",
      "27557/27557 [==============================] - 10s 365us/sample - loss: 0.0609 - val_loss: 0.0800\n",
      "Epoch 27/30\n",
      "27557/27557 [==============================] - 11s 395us/sample - loss: 0.0606 - val_loss: 0.0863\n",
      "Epoch 28/30\n",
      "27557/27557 [==============================] - 11s 404us/sample - loss: 0.0623 - val_loss: 0.0787\n",
      "Epoch 29/30\n",
      "27557/27557 [==============================] - 10s 373us/sample - loss: 0.0600 - val_loss: 0.0819\n",
      "Epoch 30/30\n",
      "27557/27557 [==============================] - 12s 436us/sample - loss: 0.0606 - val_loss: 0.0811\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization_7 (Normalizati (None, 120, 10)           21        \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 44        \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 884\n",
      "Non-trainable params: 21\n",
      "_________________________________________________________________\n",
      "Train on 26452 samples, validate on 6614 samples\n",
      "Epoch 1/30\n",
      "26452/26452 [==============================] - 17s 654us/sample - loss: 0.6187 - val_loss: 0.3632\n",
      "Epoch 2/30\n",
      "26452/26452 [==============================] - 14s 525us/sample - loss: 0.2461 - val_loss: 0.1899\n",
      "Epoch 3/30\n",
      "26452/26452 [==============================] - 10s 371us/sample - loss: 0.1462 - val_loss: 0.1402\n",
      "Epoch 4/30\n",
      "26452/26452 [==============================] - 10s 373us/sample - loss: 0.1137 - val_loss: 0.1193\n",
      "Epoch 5/30\n",
      "26452/26452 [==============================] - 11s 402us/sample - loss: 0.0985 - val_loss: 0.1107\n",
      "Epoch 6/30\n",
      "26452/26452 [==============================] - 11s 434us/sample - loss: 0.0885 - val_loss: 0.1015\n",
      "Epoch 7/30\n",
      "26452/26452 [==============================] - 11s 417us/sample - loss: 0.0831 - val_loss: 0.0989\n",
      "Epoch 8/30\n",
      "26452/26452 [==============================] - 11s 404us/sample - loss: 0.0794 - val_loss: 0.0995\n",
      "Epoch 9/30\n",
      "26452/26452 [==============================] - 14s 525us/sample - loss: 0.0767 - val_loss: 0.0943\n",
      "Epoch 10/30\n",
      "26452/26452 [==============================] - 12s 453us/sample - loss: 0.0740 - val_loss: 0.0924\n",
      "Epoch 11/30\n",
      "26452/26452 [==============================] - 10s 394us/sample - loss: 0.0725 - val_loss: 0.0938\n",
      "Epoch 12/30\n",
      "26452/26452 [==============================] - 10s 363us/sample - loss: 0.0710 - val_loss: 0.0914\n",
      "Epoch 13/30\n",
      "26452/26452 [==============================] - 10s 374us/sample - loss: 0.0695 - val_loss: 0.0933\n",
      "Epoch 14/30\n",
      "26452/26452 [==============================] - 9s 357us/sample - loss: 0.0691 - val_loss: 0.0889\n",
      "Epoch 15/30\n",
      "26452/26452 [==============================] - 11s 403us/sample - loss: 0.0671 - val_loss: 0.0899\n",
      "Epoch 16/30\n",
      "26452/26452 [==============================] - 10s 397us/sample - loss: 0.0668 - val_loss: 0.0906\n",
      "Epoch 17/30\n",
      "26452/26452 [==============================] - 10s 367us/sample - loss: 0.0671 - val_loss: 0.0871\n",
      "Epoch 18/30\n",
      "26452/26452 [==============================] - 10s 365us/sample - loss: 0.0656 - val_loss: 0.0866\n",
      "Epoch 19/30\n",
      "26452/26452 [==============================] - 10s 378us/sample - loss: 0.0642 - val_loss: 0.0887\n",
      "Epoch 20/30\n",
      "26452/26452 [==============================] - 10s 371us/sample - loss: 0.0636 - val_loss: 0.0916\n",
      "Epoch 21/30\n",
      "26452/26452 [==============================] - 10s 391us/sample - loss: 0.0629 - val_loss: 0.0866\n",
      "Epoch 22/30\n",
      "26452/26452 [==============================] - 13s 490us/sample - loss: 0.0626 - val_loss: 0.0849\n",
      "Epoch 23/30\n",
      "26452/26452 [==============================] - 11s 433us/sample - loss: 0.0613 - val_loss: 0.0861\n",
      "Epoch 24/30\n",
      "26452/26452 [==============================] - 12s 445us/sample - loss: 0.0611 - val_loss: 0.0858\n",
      "Epoch 25/30\n",
      "26452/26452 [==============================] - 11s 405us/sample - loss: 0.0607 - val_loss: 0.0853\n",
      "Epoch 26/30\n",
      "26452/26452 [==============================] - 12s 455us/sample - loss: 0.0614 - val_loss: 0.0921\n",
      "Epoch 27/30\n",
      "26452/26452 [==============================] - 11s 411us/sample - loss: 0.0598 - val_loss: 0.0824\n",
      "Epoch 28/30\n",
      "26452/26452 [==============================] - 11s 428us/sample - loss: 0.0601 - val_loss: 0.0829\n",
      "Epoch 29/30\n",
      "26452/26452 [==============================] - 12s 453us/sample - loss: 0.0586 - val_loss: 0.0861\n",
      "Epoch 30/30\n",
      "26452/26452 [==============================] - 12s 465us/sample - loss: 0.0588 - val_loss: 0.0825\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization_8 (Normalizati (None, 120, 10)           21        \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 44        \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 884\n",
      "Non-trainable params: 21\n",
      "_________________________________________________________________\n",
      "Train on 27743 samples, validate on 6936 samples\n",
      "Epoch 1/30\n",
      "27743/27743 [==============================] - 14s 504us/sample - loss: 0.6038 - val_loss: 0.4747\n",
      "Epoch 2/30\n",
      "27743/27743 [==============================] - 11s 387us/sample - loss: 0.4406 - val_loss: 0.4288\n",
      "Epoch 3/30\n",
      "27743/27743 [==============================] - 12s 426us/sample - loss: 0.4011 - val_loss: 0.4141\n",
      "Epoch 4/30\n",
      "27743/27743 [==============================] - 10s 378us/sample - loss: 0.3904 - val_loss: 0.4070\n",
      "Epoch 5/30\n",
      "27743/27743 [==============================] - 10s 354us/sample - loss: 0.3853 - val_loss: 0.4037\n",
      "Epoch 6/30\n",
      "27743/27743 [==============================] - 10s 365us/sample - loss: 0.3828 - val_loss: 0.4024\n",
      "Epoch 7/30\n",
      "27743/27743 [==============================] - 11s 401us/sample - loss: 0.3815 - val_loss: 0.4019\n",
      "Epoch 8/30\n",
      "27743/27743 [==============================] - 11s 395us/sample - loss: 0.3808 - val_loss: 0.4013\n",
      "Epoch 9/30\n",
      "27743/27743 [==============================] - 11s 414us/sample - loss: 0.3801 - val_loss: 0.4011\n",
      "Epoch 10/30\n",
      "27743/27743 [==============================] - 11s 399us/sample - loss: 0.3797 - val_loss: 0.4009\n",
      "Epoch 11/30\n",
      "27743/27743 [==============================] - 11s 400us/sample - loss: 0.3791 - val_loss: 0.4004\n",
      "Epoch 12/30\n",
      "27743/27743 [==============================] - 11s 393us/sample - loss: 0.3786 - val_loss: 0.3990\n",
      "Epoch 13/30\n",
      "27743/27743 [==============================] - 11s 406us/sample - loss: 0.3781 - val_loss: 0.3983\n",
      "Epoch 14/30\n",
      "27743/27743 [==============================] - 11s 408us/sample - loss: 0.3776 - val_loss: 0.3989\n",
      "Epoch 15/30\n",
      "27743/27743 [==============================] - 11s 393us/sample - loss: 0.3771 - val_loss: 0.3984\n",
      "Epoch 16/30\n",
      "27743/27743 [==============================] - 11s 386us/sample - loss: 0.3766 - val_loss: 0.3976\n",
      "Epoch 17/30\n",
      "27743/27743 [==============================] - 12s 421us/sample - loss: 0.3761 - val_loss: 0.3976\n",
      "Epoch 18/30\n",
      "27743/27743 [==============================] - 13s 456us/sample - loss: 0.3756 - val_loss: 0.3962\n",
      "Epoch 19/30\n",
      "27743/27743 [==============================] - 11s 381us/sample - loss: 0.3751 - val_loss: 0.3964\n",
      "Epoch 20/30\n",
      "27743/27743 [==============================] - 12s 449us/sample - loss: 0.3745 - val_loss: 0.3960\n",
      "Epoch 21/30\n",
      "27743/27743 [==============================] - 12s 419us/sample - loss: 0.3739 - val_loss: 0.3955\n",
      "Epoch 22/30\n",
      "27743/27743 [==============================] - 12s 421us/sample - loss: 0.3733 - val_loss: 0.3951\n",
      "Epoch 23/30\n",
      "27743/27743 [==============================] - 14s 497us/sample - loss: 0.3726 - val_loss: 0.3946\n",
      "Epoch 24/30\n",
      "27743/27743 [==============================] - 12s 443us/sample - loss: 0.3720 - val_loss: 0.3947\n",
      "Epoch 25/30\n",
      "27743/27743 [==============================] - 11s 387us/sample - loss: 0.3713 - val_loss: 0.3934\n",
      "Epoch 26/30\n",
      "27743/27743 [==============================] - 13s 454us/sample - loss: 0.3705 - val_loss: 0.3921\n",
      "Epoch 27/30\n",
      "27743/27743 [==============================] - 12s 446us/sample - loss: 0.3695 - val_loss: 0.3905\n",
      "Epoch 28/30\n",
      "27743/27743 [==============================] - 10s 369us/sample - loss: 0.3684 - val_loss: 0.3899\n",
      "Epoch 29/30\n",
      "27743/27743 [==============================] - 10s 375us/sample - loss: 0.3672 - val_loss: 0.3885\n",
      "Epoch 30/30\n",
      "27743/27743 [==============================] - 12s 417us/sample - loss: 0.3657 - val_loss: 0.3875\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization_9 (Normalizati (None, 120, 10)           21        \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4)                 44        \n",
      "=================================================================\n",
      "Total params: 905\n",
      "Trainable params: 884\n",
      "Non-trainable params: 21\n",
      "_________________________________________________________________\n",
      "Train on 26624 samples, validate on 6656 samples\n",
      "Epoch 1/30\n",
      "26624/26624 [==============================] - 15s 575us/sample - loss: 0.5150 - val_loss: 0.2802\n",
      "Epoch 2/30\n",
      "26624/26624 [==============================] - 10s 371us/sample - loss: 0.2025 - val_loss: 0.1796\n",
      "Epoch 3/30\n",
      "26624/26624 [==============================] - 8s 317us/sample - loss: 0.1418 - val_loss: 0.1381\n",
      "Epoch 4/30\n",
      "26624/26624 [==============================] - 13s 482us/sample - loss: 0.1146 - val_loss: 0.1151\n",
      "Epoch 5/30\n",
      "26624/26624 [==============================] - 10s 377us/sample - loss: 0.0966 - val_loss: 0.0996\n",
      "Epoch 6/30\n",
      "26624/26624 [==============================] - 9s 350us/sample - loss: 0.0855 - val_loss: 0.0962\n",
      "Epoch 7/30\n",
      "26624/26624 [==============================] - 8s 307us/sample - loss: 0.0789 - val_loss: 0.0878\n",
      "Epoch 8/30\n",
      "26624/26624 [==============================] - 8s 301us/sample - loss: 0.0743 - val_loss: 0.0826\n",
      "Epoch 9/30\n",
      "26624/26624 [==============================] - 9s 330us/sample - loss: 0.0719 - val_loss: 0.0834\n",
      "Epoch 10/30\n",
      "26624/26624 [==============================] - 9s 322us/sample - loss: 0.0692 - val_loss: 0.0814\n",
      "Epoch 11/30\n",
      "26624/26624 [==============================] - 9s 340us/sample - loss: 0.0670 - val_loss: 0.0782\n",
      "Epoch 12/30\n",
      "26624/26624 [==============================] - 9s 353us/sample - loss: 0.0667 - val_loss: 0.0785\n",
      "Epoch 13/30\n",
      "26624/26624 [==============================] - 9s 336us/sample - loss: 0.0641 - val_loss: 0.0774\n",
      "Epoch 14/30\n",
      "26624/26624 [==============================] - 8s 310us/sample - loss: 0.0634 - val_loss: 0.0752\n",
      "Epoch 15/30\n",
      "26624/26624 [==============================] - 8s 285us/sample - loss: 0.0627 - val_loss: 0.0734\n",
      "Epoch 16/30\n",
      "26624/26624 [==============================] - 8s 302us/sample - loss: 0.0617 - val_loss: 0.0737\n",
      "Epoch 17/30\n",
      "26624/26624 [==============================] - 8s 300us/sample - loss: 0.0617 - val_loss: 0.0728\n",
      "Epoch 18/30\n",
      "26624/26624 [==============================] - 8s 296us/sample - loss: 0.0611 - val_loss: 0.0738\n",
      "Epoch 19/30\n",
      "26624/26624 [==============================] - 8s 286us/sample - loss: 0.0605 - val_loss: 0.0721\n",
      "Epoch 20/30\n",
      "26624/26624 [==============================] - 8s 304us/sample - loss: 0.0595 - val_loss: 0.0741\n",
      "Epoch 21/30\n",
      "26624/26624 [==============================] - 8s 285us/sample - loss: 0.0590 - val_loss: 0.0726\n",
      "Epoch 22/30\n",
      "26624/26624 [==============================] - 8s 286us/sample - loss: 0.0590 - val_loss: 0.0729\n",
      "Epoch 23/30\n",
      "26624/26624 [==============================] - 8s 285us/sample - loss: 0.0582 - val_loss: 0.0701\n",
      "Epoch 24/30\n",
      "26624/26624 [==============================] - 8s 286us/sample - loss: 0.0582 - val_loss: 0.0714\n",
      "Epoch 25/30\n",
      "26624/26624 [==============================] - 8s 285us/sample - loss: 0.0573 - val_loss: 0.0733\n",
      "Epoch 26/30\n",
      "26624/26624 [==============================] - 8s 284us/sample - loss: 0.0572 - val_loss: 0.0684\n",
      "Epoch 27/30\n",
      "26624/26624 [==============================] - 8s 294us/sample - loss: 0.0572 - val_loss: 0.0706\n",
      "Epoch 28/30\n",
      "26624/26624 [==============================] - 8s 301us/sample - loss: 0.0566 - val_loss: 0.0768\n",
      "Epoch 29/30\n",
      "26624/26624 [==============================] - 8s 302us/sample - loss: 0.0567 - val_loss: 0.0729\n",
      "Epoch 30/30\n",
      "26624/26624 [==============================] - 8s 286us/sample - loss: 0.0565 - val_loss: 0.0686\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for model_config in hyperparameter_configs:\n",
    "    current_result = {}\n",
    "    current_result['model_config'] = model_config\n",
    "    current_result['left_validation_rmse'] = []\n",
    "    current_result['right_validation_rmse'] = []\n",
    "    for trial in np.arange(1,11):\n",
    "        dataset = cnn_extract_features(data_list, model_config['window_size'], trial)\n",
    "#         dataset = cnn_train_test_split(trial, model_config['window_size'])\n",
    "        model = lstm_model(sequence_length=model_config['window_size'],\n",
    "                          n_features=10, \n",
    "                           lstm_config=model_config['lstm'],\n",
    "                           dense_config=model_config['dense'],\n",
    "                           optim_config=model_config['optimizer'],\n",
    "                           X_train=dataset['X_train'].squeeze())\n",
    "        model.summary()\n",
    "        early_stopping_callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0)\n",
    "        model_hist = model.fit(dataset['X_train'].squeeze(), dataset['y_train'].squeeze(), epochs=30, batch_size=128, verbose=1, validation_split=0.2, shuffle=True, callbacks= [early_stopping_callback])\n",
    "        # model.save('test_rnn_model')\n",
    "\n",
    "        predictions = model.predict(dataset['X_test'].squeeze())\n",
    "        left_rmse, right_rmse = custom_rmse(dataset['y_test'].squeeze(), predictions)\n",
    "\n",
    "        current_result['left_validation_rmse'].append(left_rmse)\n",
    "        current_result['right_validation_rmse'].append(right_rmse)\n",
    "    results.append(current_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "25P-wi-e9_6t"
   },
   "outputs": [],
   "source": [
    "for model in results:\n",
    "    model['left_rmse_mean'] = np.mean(model['left_validation_rmse'])\n",
    "    model['right_rmse_mean'] = np.mean(model['right_validation_rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hHPPcgbUYMUJ"
   },
   "outputs": [],
   "source": [
    "def mapper(x):\n",
    "    out = {}\n",
    "    out['window_size'] = x['model_config']['window_size']\n",
    "    for key in x['model_config']['lstm'].keys():\n",
    "        out['lstm_{}'.format(key)] = x['model_config']['lstm'][key]\n",
    "\n",
    "    for key in x['model_config']['dense'].keys():\n",
    "        out['dense_{}'.format(key)] = x['model_config']['dense'][key]\n",
    "\n",
    "    for key in x['model_config']['optimizer'].keys():\n",
    "        out['optim_{}'.format(key)] = x['model_config']['optimizer'][key]\n",
    "\n",
    "    out['left_rmse_mean'] = x['left_rmse_mean']\n",
    "    out['right_rmse_mean'] = x['right_rmse_mean']\n",
    "    return out\n",
    "  \n",
    "input_df = list(map(mapper, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3PZYXyYTZLiG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window_size</th>\n",
       "      <th>lstm_units</th>\n",
       "      <th>lstm_activation</th>\n",
       "      <th>lstm_recurrent_regularizer</th>\n",
       "      <th>lstm_kernel_regularizer</th>\n",
       "      <th>dense_activation</th>\n",
       "      <th>dense_kernel_regularizer</th>\n",
       "      <th>optim_loss</th>\n",
       "      <th>optim_optimizer</th>\n",
       "      <th>left_rmse_mean</th>\n",
       "      <th>right_rmse_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>10</td>\n",
       "      <td>relu</td>\n",
       "      <td>l2</td>\n",
       "      <td>l2</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l2</td>\n",
       "      <td>mean_absolute_error</td>\n",
       "      <td>adam</td>\n",
       "      <td>2.655973</td>\n",
       "      <td>2.720643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   window_size  lstm_units lstm_activation lstm_recurrent_regularizer  \\\n",
       "0          120          10            relu                         l2   \n",
       "\n",
       "  lstm_kernel_regularizer dense_activation dense_kernel_regularizer  \\\n",
       "0                      l2             tanh                       l2   \n",
       "\n",
       "            optim_loss optim_optimizer  left_rmse_mean  right_rmse_mean  \n",
       "0  mean_absolute_error            adam        2.655973         2.720643  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(results))\n",
    "df_results = pd.DataFrame(input_df)\n",
    "display(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yHrOPVmfOhZa"
   },
   "outputs": [],
   "source": [
    "df_results.to_csv('results_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_Wt7PRujQDT"
   },
   "outputs": [],
   "source": [
    "# print(left_rmse)\n",
    "# print(right_rmse)\n",
    "# plt.figure(1)\n",
    "# plt.plot(gp['left_true'])\n",
    "# plt.plot(gp['left_pred'])\n",
    "# plt.legend(['GT', 'Pred'])\n",
    "# plt.show()\n",
    "\n",
    "# Define list of hyperparameter options\n",
    "# For each hyperparameter combination\n",
    "#   Create model using hyperparameters\n",
    "#   Train model\n",
    "#   Store hyperparameters and validation loss\n",
    "#   Store plot of training/validation loss over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vakvCACojQDc"
   },
   "outputs": [],
   "source": [
    "# n_epochs = len(model_hist.history['loss'])\n",
    "# epochs = np.arange(1,n_epochs+1)\n",
    "# plt.figure(1)\n",
    "# plt.plot(epochs, model_hist.history['loss'])\n",
    "# plt.plot(epochs, model_hist.history['val_loss'])\n",
    "# plt.legend(['Training Loss', 'Validation Loss'])\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_Gait_Phase_Sweep.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
